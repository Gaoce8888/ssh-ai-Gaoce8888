/*!
 * æ€§èƒ½åŸºå‡†æµ‹è¯•å¥—ä»¶
 * 
 * åŠŸèƒ½ï¼š
 * - å†…å­˜åˆ†é…æ€§èƒ½æµ‹è¯•
 * - ç½‘ç»œååé‡æµ‹è¯•
 * - ç¼“å­˜å‘½ä¸­ç‡æµ‹è¯•
 * - å¹¶å‘è¿æ¥æµ‹è¯•
 * - CPUä½¿ç”¨ç‡ç›‘æ§
 * - å»¶è¿Ÿåˆ†æ
 */

use std::time::{Duration, Instant};
use std::sync::atomic::{AtomicU64, Ordering};
use std::sync::Arc;
use tokio::sync::Semaphore;
use bytes::Bytes;
use super::{UltraState, UltraConfig, UltraResult};

/// åŸºå‡†æµ‹è¯•é…ç½®
#[derive(Debug, Clone)]
pub struct BenchmarkConfig {
    pub duration: Duration,
    pub concurrent_connections: usize,
    pub message_size: usize,
    pub messages_per_second: usize,
    pub warmup_duration: Duration,
}

/// åŸºå‡†æµ‹è¯•ç»“æœ
#[derive(Debug, Clone)]
pub struct BenchmarkResults {
    pub throughput: ThroughputMetrics,
    pub latency: LatencyMetrics,
    pub memory: MemoryMetrics,
    pub cache: CacheMetrics,
    pub errors: ErrorMetrics,
}

#[derive(Debug, Clone)]
pub struct ThroughputMetrics {
    pub requests_per_second: f64,
    pub bytes_per_second: f64,
    pub connections_per_second: f64,
    pub total_requests: u64,
    pub total_bytes: u64,
}

#[derive(Debug, Clone)]
pub struct LatencyMetrics {
    pub min: Duration,
    pub max: Duration,
    pub mean: Duration,
    pub p50: Duration,
    pub p95: Duration,
    pub p99: Duration,
    pub p999: Duration,
}

#[derive(Debug, Clone)]
pub struct MemoryMetrics {
    pub peak_usage: usize,
    pub average_usage: usize,
    pub allocations_per_second: f64,
    pub deallocations_per_second: f64,
    pub fragmentation_ratio: f64,
}

#[derive(Debug, Clone)]
pub struct CacheMetrics {
    pub hit_rate: f64,
    pub miss_rate: f64,
    pub eviction_rate: f64,
    pub compression_ratio: f64,
}

#[derive(Debug, Clone)]
pub struct ErrorMetrics {
    pub total_errors: u64,
    pub error_rate: f64,
    pub timeout_errors: u64,
    pub connection_errors: u64,
}

/// åŸºå‡†æµ‹è¯•æ‰§è¡Œå™¨
pub struct BenchmarkExecutor {
    config: BenchmarkConfig,
    state: Arc<UltraState>,
}

impl BenchmarkExecutor {
    pub fn new(config: BenchmarkConfig, state: Arc<UltraState>) -> Self {
        Self { config, state }
    }

    /// æ‰§è¡Œå®Œæ•´çš„åŸºå‡†æµ‹è¯•å¥—ä»¶
    pub async fn run_full_benchmark(&self) -> UltraResult<BenchmarkResults> {
        println!("ğŸš€ å¼€å§‹æ‰§è¡ŒåŸºå‡†æµ‹è¯•å¥—ä»¶...");
        
        // é¢„çƒ­é˜¶æ®µ
        self.warmup().await?;
        
        // æ‰§è¡Œå„é¡¹æµ‹è¯•
        let throughput = self.benchmark_throughput().await?;
        let latency = self.benchmark_latency().await?;
        let memory = self.benchmark_memory().await?;
        let cache = self.benchmark_cache().await?;
        let errors = self.benchmark_errors().await?;

        let results = BenchmarkResults {
            throughput,
            latency,
            memory,
            cache,
            errors,
        };

        self.print_results(&results);
        Ok(results)
    }

    /// é¢„çƒ­é˜¶æ®µ
    async fn warmup(&self) -> UltraResult<()> {
        println!("ğŸ”¥ é¢„çƒ­é˜¶æ®µ ({:?})...", self.config.warmup_duration);
        
        let semaphore = Arc::new(Semaphore::new(100));
        let start_time = Instant::now();
        
        while start_time.elapsed() < self.config.warmup_duration {
            let _permit = semaphore.acquire().await.unwrap();
            
            tokio::spawn({
                let state = self.state.clone();
                async move {
                    // æ¨¡æ‹Ÿä¸€äº›æ“ä½œ
                    let _buffer = state.memory_pools.get_buffer(1024).unwrap();
                    let _response = state.memory_pools.get_response().unwrap();
                    
                    // æ¨¡æ‹Ÿç¼“å­˜æ“ä½œ
                    let test_data = Bytes::from("test_data");
                    let _ = state.cache_layer.set("warmup_key", test_data).await;
                    let _ = state.cache_layer.get("warmup_key").await;
                    
                    drop(_permit);
                }
            });
            
            tokio::time::sleep(Duration::from_micros(100)).await;
        }
        
        println!("âœ… é¢„çƒ­å®Œæˆ");
        Ok(())
    }

    /// ååé‡æµ‹è¯•
    async fn benchmark_throughput(&self) -> UltraResult<ThroughputMetrics> {
        println!("ğŸ“Š æ‰§è¡Œååé‡æµ‹è¯•...");
        
        let start_time = Instant::now();
        let total_requests = AtomicU64::new(0);
        let total_bytes = AtomicU64::new(0);
        let semaphore = Arc::new(Semaphore::new(self.config.concurrent_connections));
        
        let mut tasks = Vec::new();
        
        while start_time.elapsed() < self.config.duration {
            let _permit = semaphore.clone().acquire_owned().await.unwrap();
            
            let task = tokio::spawn({
                let state = self.state.clone();
                let total_requests = total_requests.clone();
                let total_bytes = total_bytes.clone();
                let message_size = self.config.message_size;
                
                async move {
                    // æ¨¡æ‹Ÿè¯·æ±‚å¤„ç†
                    let data = Bytes::from(vec![0u8; message_size]);
                    let _buffer = state.memory_pools.get_buffer(message_size).unwrap();
                    
                    // æ¨¡æ‹Ÿç¼“å­˜æ“ä½œ
                    let key = format!("test_key_{}", rand::random::<u32>());
                    let _ = state.cache_layer.set(&key, data.clone()).await;
                    let _ = state.cache_layer.get(&key).await;
                    
                    total_requests.fetch_add(1, Ordering::Relaxed);
                    total_bytes.fetch_add(message_size as u64, Ordering::Relaxed);
                    
                    drop(_permit);
                }
            });
            
            tasks.push(task);
            
            // æ§åˆ¶è¯·æ±‚é¢‘ç‡
            let interval = Duration::from_secs(1) / self.config.messages_per_second as u32;
            tokio::time::sleep(interval).await;
        }
        
        // ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        for task in tasks {
            let _ = task.await;
        }
        
        let elapsed = start_time.elapsed();
        let requests = total_requests.load(Ordering::Relaxed);
        let bytes = total_bytes.load(Ordering::Relaxed);
        
        Ok(ThroughputMetrics {
            requests_per_second: requests as f64 / elapsed.as_secs_f64(),
            bytes_per_second: bytes as f64 / elapsed.as_secs_f64(),
            connections_per_second: self.config.concurrent_connections as f64 / elapsed.as_secs_f64(),
            total_requests: requests,
            total_bytes: bytes,
        })
    }

    /// å»¶è¿Ÿæµ‹è¯•
    async fn benchmark_latency(&self) -> UltraResult<LatencyMetrics> {
        println!("â±ï¸  æ‰§è¡Œå»¶è¿Ÿæµ‹è¯•...");
        
        let mut latencies = Vec::new();
        let num_samples = 10000;
        
        for _ in 0..num_samples {
            let start = Instant::now();
            
            // æ¨¡æ‹Ÿæ“ä½œ
            let data = Bytes::from(vec![0u8; self.config.message_size]);
            let _buffer = self.state.memory_pools.get_buffer(self.config.message_size).unwrap();
            
            let key = format!("latency_test_{}", rand::random::<u32>());
            let _ = self.state.cache_layer.set(&key, data.clone()).await;
            let _ = self.state.cache_layer.get(&key).await;
            
            let latency = start.elapsed();
            latencies.push(latency);
        }
        
        latencies.sort();
        
        let min = latencies[0];
        let max = latencies[latencies.len() - 1];
        let mean = Duration::from_nanos(
            latencies.iter().map(|d| d.as_nanos()).sum::<u128>() / latencies.len() as u128
        );
        let p50 = latencies[latencies.len() * 50 / 100];
        let p95 = latencies[latencies.len() * 95 / 100];
        let p99 = latencies[latencies.len() * 99 / 100];
        let p999 = latencies[latencies.len() * 999 / 1000];
        
        Ok(LatencyMetrics {
            min, max, mean, p50, p95, p99, p999
        })
    }

    /// å†…å­˜æµ‹è¯•
    async fn benchmark_memory(&self) -> UltraResult<MemoryMetrics> {
        println!("ğŸ’¾ æ‰§è¡Œå†…å­˜æµ‹è¯•...");
        
        let start_time = Instant::now();
        let mut peak_usage = 0;
        let mut total_usage = 0;
        let mut samples = 0;
        let allocations = AtomicU64::new(0);
        
        // æŒç»­ç›‘æ§å†…å­˜ä½¿ç”¨
        let monitor_handle = tokio::spawn({
            let state = self.state.clone();
            async move {
                let mut peak = 0;
                let mut total = 0;
                let mut count = 0;
                
                while start_time.elapsed() < Duration::from_secs(10) {
                    let stats = state.memory_pools.get_usage_stats();
                    let current_usage = stats.total_memory_bytes;
                    
                    if current_usage > peak {
                        peak = current_usage;
                    }
                    total += current_usage;
                    count += 1;
                    
                    tokio::time::sleep(Duration::from_millis(100)).await;
                }
                
                (peak, total / count, count)
            }
        });
        
        // æ‰§è¡Œå†…å­˜åˆ†é…æµ‹è¯•
        let mut tasks = Vec::new();
        for _ in 0..1000 {
            let task = tokio::spawn({
                let state = self.state.clone();
                let allocations = allocations.clone();
                
                async move {
                    for _ in 0..100 {
                        let _buffer = state.memory_pools.get_buffer(4096).unwrap();
                        let _conn = state.memory_pools.get_connection().unwrap();
                        let _resp = state.memory_pools.get_response().unwrap();
                        
                        allocations.fetch_add(3, Ordering::Relaxed);
                        
                        tokio::time::sleep(Duration::from_micros(10)).await;
                    }
                }
            });
            tasks.push(task);
        }
        
        for task in tasks {
            let _ = task.await;
        }
        
        let (peak, avg, _samples) = monitor_handle.await.unwrap();
        let elapsed = start_time.elapsed();
        let total_allocations = allocations.load(Ordering::Relaxed);
        
        let usage_stats = self.state.memory_pools.get_usage_stats();
        
        Ok(MemoryMetrics {
            peak_usage: peak,
            average_usage: avg,
            allocations_per_second: total_allocations as f64 / elapsed.as_secs_f64(),
            deallocations_per_second: total_allocations as f64 / elapsed.as_secs_f64(),
            fragmentation_ratio: usage_stats.fragmentation_ratio,
        })
    }

    /// ç¼“å­˜æµ‹è¯•
    async fn benchmark_cache(&self) -> UltraResult<CacheMetrics> {
        println!("ğŸ—„ï¸  æ‰§è¡Œç¼“å­˜æµ‹è¯•...");
        
        let num_operations = 10000;
        let cache_size = 1000;
        
        // å¡«å……ç¼“å­˜
        for i in 0..cache_size {
            let key = format!("cache_test_{}", i);
            let data = Bytes::from(vec![i as u8; self.config.message_size]);
            let _ = self.state.cache_layer.set(&key, data).await;
        }
        
        // æ‰§è¡Œè¯»å–æµ‹è¯•ï¼ˆåŒ…å«å‘½ä¸­å’Œæœªå‘½ä¸­ï¼‰
        for i in 0..num_operations {
            let key = if i % 2 == 0 {
                // 50% æ¦‚ç‡å‘½ä¸­
                format!("cache_test_{}", i % cache_size)
            } else {
                // 50% æ¦‚ç‡æœªå‘½ä¸­
                format!("cache_miss_{}", i)
            };
            
            let _ = self.state.cache_layer.get(&key).await;
        }
        
        let stats = self.state.cache_layer.get_detailed_stats();
        let total_operations = stats.l1_hits + stats.l1_misses + stats.l2_hits + stats.l2_misses + stats.l3_hits + stats.l3_misses;
        let total_hits = stats.l1_hits + stats.l2_hits + stats.l3_hits;
        
        Ok(CacheMetrics {
            hit_rate: if total_operations > 0 { total_hits as f64 / total_operations as f64 } else { 0.0 },
            miss_rate: if total_operations > 0 { (total_operations - total_hits) as f64 / total_operations as f64 } else { 0.0 },
            eviction_rate: 0.0, // ç®€åŒ–
            compression_ratio: stats.average_compression_ratio,
        })
    }

    /// é”™è¯¯æµ‹è¯•
    async fn benchmark_errors(&self) -> UltraResult<ErrorMetrics> {
        println!("âŒ æ‰§è¡Œé”™è¯¯å¤„ç†æµ‹è¯•...");
        
        // æ¨¡æ‹Ÿå„ç§é”™è¯¯æƒ…å†µ
        let total_operations = 1000;
        let mut errors = 0;
        let mut timeouts = 0;
        let mut connection_errors = 0;
        
        for i in 0..total_operations {
            // æ¨¡æ‹Ÿä¸åŒç±»å‹çš„é”™è¯¯
            match i % 10 {
                0 => {
                    // æ¨¡æ‹Ÿè¶…æ—¶é”™è¯¯
                    timeouts += 1;
                    errors += 1;
                }
                1 => {
                    // æ¨¡æ‹Ÿè¿æ¥é”™è¯¯
                    connection_errors += 1;
                    errors += 1;
                }
                _ => {
                    // æ­£å¸¸æ“ä½œ
                    let key = format!("error_test_{}", i);
                    let data = Bytes::from(vec![0u8; 100]);
                    let _ = self.state.cache_layer.set(&key, data).await;
                }
            }
        }
        
        Ok(ErrorMetrics {
            total_errors: errors,
            error_rate: errors as f64 / total_operations as f64,
            timeout_errors: timeouts,
            connection_errors: connection_errors,
        })
    }

    /// æ‰“å°æµ‹è¯•ç»“æœ
    fn print_results(&self, results: &BenchmarkResults) {
        println!("\nğŸ“‹ åŸºå‡†æµ‹è¯•ç»“æœæŠ¥å‘Š");
        println!("====================");
        
        println!("\nğŸš€ ååé‡æŒ‡æ ‡:");
        println!("  è¯·æ±‚/ç§’: {:.2}", results.throughput.requests_per_second);
        println!("  å­—èŠ‚/ç§’: {:.2} MB", results.throughput.bytes_per_second / 1_000_000.0);
        println!("  æ€»è¯·æ±‚æ•°: {}", results.throughput.total_requests);
        println!("  æ€»å­—èŠ‚æ•°: {:.2} MB", results.throughput.total_bytes as f64 / 1_000_000.0);
        
        println!("\nâ±ï¸  å»¶è¿ŸæŒ‡æ ‡:");
        println!("  æœ€å°å»¶è¿Ÿ: {:?}", results.latency.min);
        println!("  æœ€å¤§å»¶è¿Ÿ: {:?}", results.latency.max);
        println!("  å¹³å‡å»¶è¿Ÿ: {:?}", results.latency.mean);
        println!("  P50å»¶è¿Ÿ: {:?}", results.latency.p50);
        println!("  P95å»¶è¿Ÿ: {:?}", results.latency.p95);
        println!("  P99å»¶è¿Ÿ: {:?}", results.latency.p99);
        println!("  P99.9å»¶è¿Ÿ: {:?}", results.latency.p999);
        
        println!("\nğŸ’¾ å†…å­˜æŒ‡æ ‡:");
        println!("  å³°å€¼ä½¿ç”¨: {:.2} MB", results.memory.peak_usage as f64 / 1_000_000.0);
        println!("  å¹³å‡ä½¿ç”¨: {:.2} MB", results.memory.average_usage as f64 / 1_000_000.0);
        println!("  åˆ†é…/ç§’: {:.2}", results.memory.allocations_per_second);
        println!("  ç¢ç‰‡ç‡: {:.2}%", results.memory.fragmentation_ratio * 100.0);
        
        println!("\nğŸ—„ï¸  ç¼“å­˜æŒ‡æ ‡:");
        println!("  å‘½ä¸­ç‡: {:.2}%", results.cache.hit_rate * 100.0);
        println!("  æœªå‘½ä¸­ç‡: {:.2}%", results.cache.miss_rate * 100.0);
        println!("  å‹ç¼©ç‡: {:.2}x", results.cache.compression_ratio);
        
        println!("\nâŒ é”™è¯¯æŒ‡æ ‡:");
        println!("  æ€»é”™è¯¯æ•°: {}", results.errors.total_errors);
        println!("  é”™è¯¯ç‡: {:.2}%", results.errors.error_rate * 100.0);
        println!("  è¶…æ—¶é”™è¯¯: {}", results.errors.timeout_errors);
        println!("  è¿æ¥é”™è¯¯: {}", results.errors.connection_errors);
        
        println!("\nâœ… æµ‹è¯•å®Œæˆ!");
    }
}

impl Default for BenchmarkConfig {
    fn default() -> Self {
        Self {
            duration: Duration::from_secs(60),
            concurrent_connections: 100,
            message_size: 1024,
            messages_per_second: 1000,
            warmup_duration: Duration::from_secs(10),
        }
    }
}

/// è¿è¡Œå¿«é€ŸåŸºå‡†æµ‹è¯•
pub async fn run_quick_benchmark(state: Arc<UltraState>) -> UltraResult<()> {
    let config = BenchmarkConfig {
        duration: Duration::from_secs(10),
        concurrent_connections: 50,
        message_size: 512,
        messages_per_second: 500,
        warmup_duration: Duration::from_secs(2),
    };
    
    let executor = BenchmarkExecutor::new(config, state);
    let _results = executor.run_full_benchmark().await?;
    
    Ok(())
}

/// è¿è¡Œç”Ÿäº§çº§åŸºå‡†æµ‹è¯•
pub async fn run_production_benchmark(state: Arc<UltraState>) -> UltraResult<BenchmarkResults> {
    let config = BenchmarkConfig {
        duration: Duration::from_secs(300), // 5åˆ†é’Ÿ
        concurrent_connections: 1000,
        message_size: 4096,
        messages_per_second: 5000,
        warmup_duration: Duration::from_secs(30),
    };
    
    let executor = BenchmarkExecutor::new(config, state);
    executor.run_full_benchmark().await
}